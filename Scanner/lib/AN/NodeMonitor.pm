package AN::NodeMonitor;

use parent 'AN::SNMP::APC_UPS';    # inherit from AN::SNMP_APC_UPS

# _Perl_
use warnings;
use strict;
use 5.010;

use version;
our $VERSION = '1.0.0';

use Carp;
use Const::Fast;
use Data::Dumper;
use English '-no_match_vars';
use File::Basename;
use POSIX 'strftime';

use AN::Cluster;

# ======================================================================
# CLASS ATTRIBUTES
#
use Class::Tiny qw( healthfile host status first_ping_at sent_last_timeout_at
    elapsed), { cache => sub { $_[0]->read_cache_file(); }, };

# ======================================================================
# CONSTANTS
#
const my $PROG  => ( fileparse($PROGRAM_NAME) )[0];
const my $SLASH => q{/};
const my %STATUS2MSG => ( on  => 'host has power',
                          off => 'host down' );

# ======================================================================
# METHODS
#

# ----------------------------------------------------------------------
# Interface to insert db records.
#
sub insert_raw_record {
    my $self = shift;
    my ($args) = @_;

    $self->dbs->insert_raw_record($args);
    return;
}

# ----------------------------------------------------------------------
# True if healhfile reports health as 'ok', otherwise false.
#
sub healthfile_status_shows_ok {
    my $self = shift;

    state $file = $self->healthfile();

    return unless -r $file;

    open my $hf, '<', $file
        or carp "Could not open healthfile '$file'.";
    my (@line) = grep {/health/} split "\n", <$hf>;
    my $status = ( split ' = ', $line[0] )[1];
    close $hf;

    return $status eq 'ok';
}

# ----------------------------------------------------------------------
# The 'cache' file is generated by the Striker system and reports
# mechanisms to fence servers and to check power, as well as a copy of
# the hosts table from a particular node server. Read in the file and
# archive particular portions of it.
#
sub read_cache_file {
    my $self = shift;

    say "Reading cache file";
    my $args = { cgi  => { cluster => $self->confdata->{cluster} },
                 path => { striker_cache => $self->confdata->{striker_cache},
                           log           => '/dev/null',
                         }, };
    AN::Cluster::read_node_cache( $args, $self->host );
    delete $args->{cgi};
    delete $args->{path};
    delete $args->{handles};

    $self->cache($args);

    return $args;
}

# ----------------------------------------------------------------------
# Look up the IP number for a host in the 'cache' file.
#
sub get_ip {
    my $self = shift;

    my $ipmi_host = $self->host() . '.ipmi';
    my $ip        = $self->cache()->{node}{ $self->host }{hosts}{$ipmi_host};
    die( "Host @{[$self->host()]} not found in cache file ",
         Data::Dumper::Dumper( [ $self->cache() ] ), "\n" )
        unless $ip;

    return $ip->{ip};
}

# ----------------------------------------------------------------------
# Run an IPMI command, either to fetch the status or to power up the
# system.
#
sub ipmi_power_utility {
    my $self = shift;
    my ($cmd) = @_;

    my $shell = join ' ',
        ( $self->bindir . $SLASH . $self->confdata->{ipmitool},
          '-h', $self->get_ip, '-c', $cmd );

    say "Running : $shell" if $self->verbose;
    my $lines = `$shell 2>&1`;
    say join "\nIPMITOOL ==> ", split "\n", $lines
        if $self->verbose;
    return $lines;
}

# ----------------------------------------------------------------------
# Use the ipmi_power_utility to fetch the power status of a specified
# server.
#
sub fetch_host_power_status {
    my $self = shift;

    my $lines = $self->ipmi_power_utility('status');
    my ($status) = $lines =~ m{Chassis  \s Power \s is \s (\w+)}xms;
    return $status;
}

# ----------------------------------------------------------------------
# Fetch the power status of a server and verify the received message.
#
sub verify_host_down {
    my $self = shift;

    my $status = $self->fetch_host_power_status;
    my $msg = exists $STATUS2MSG{$status} && $STATUS2MSG{$status};

    $self->status($msg)
        if $msg;
    return;
}

# ----------------------------------------------------------------------
# Turn on a server using the ipmi_power_utility.
#
sub relaunch_host {
    my $self = shift;

    my $lines = $self->ipmi_power_utility('on');
    my ($status) = ( $lines =~ m{Chassis \s Power \s Control: \s (\S+)}xms );

    $self->status('host has power')
        if $status eq 'Up/On';
    return;
}

# ----------------------------------------------------------------------
#
# When we're trying to ping the host, if we haven't reached
# max_seconds_for_reboot ( let's call it MAX) yet, then keep looping
# around and trying every MAX seconds.
#
# If we have timed out, save the current_time in sent_last_timeout_at,
# and send an alert saying how long the reboot been going on. In the
# future, use the time since sent_last_timeout_at to determine when to
# timeout again. Every MAX seconds, send a new alert.
#
sub has_timed_out {
    my $self = shift;

    state $max = $self->confdata->{max_seconds_for_reboot};
    my $elapsed = time - $self->first_ping_at;

    if ( $elapsed > $max ) {
        if ( ( !$self->sent_last_timeout_at )
             || time - $self->sent_last_timeout_at > $max ) {
            $self->sent_last_timeout_at(time);
            $self->elapsed($elapsed);
            $self->status('reboot timed out');
            return;    # timed out, return total elapsed time.
        }
        else {
            $self->elapsed(0);
            return;    # false, keep running
        }
    }
}

# ----------------------------------------------------------------------
# Run a sing 'ping' command. The host is up if ping reports "0% packet
# loss". Return true for 'up', 'false' for 'not up'.
#
sub ping_host {
    my $self = shift;

    my $shell = $self->confdata->{ping} . " -c 1 " . $self->get_ip;

    say "Running : $shell" if $self->verbose;
    my @lines = split "\n", `$shell`;

    say join "\nPing -> ", @lines if $self->verbose;
    my ($num) = ( $lines[4] =~ m{([\d.]+)% \s packet \s loss}xms );

    return $num == 0;
}

# ----------------------------------------------------------------------
# There are three possibilities:
#     1) The host is still coming up. In this case, loop around for
#     another 30 seconds and try again.
#     2) The host is pingable. Report success. Yippee!
#     3) We've run out of time, and the host isn't up. Boo! Hiss!
#     Report failure. but keep trying, send message every N minutes.
#
sub ping_host_until_OK_or_timeout {
    my $self = shift;

    $self->first_ping_at(time) unless $self->first_ping_at;

    my $pingable = $self->ping_host;

    if ($pingable) {
        $self->status('host is pingable');
    }
    else {
        $self->status('reboot timed out')
            if $self->has_timed_out;
    }
    return;
}

# ----------------------------------------------------------------------
# Insert an alerts table record reporting the current status of the
# node server.
#
sub report_host_status_to_boss {
    my $self = shift;

    my $host = $self->host;
    my $status = (   $self->status_host_is_pingable ? 'OK'
                   : $self->status_host_timed_out   ? 'TIMEOUT'
                   :                                  'unknown' );
    if ( $status eq 'unknown' ) {
        carp "unknown status @{[ $self->status()]}.";
        $status = 'DEAD';
    }

    my $msg_args = "host=$host";
    $msg_args .= "elapsed=" . $self->elapsed
        if $self->elapsed;
    my $record = { table              => $self->confdata->{db}{table}{alerts},
                   with_node_table_id => 'node_id',
                   args               => {
                             value             => $self->host,
                             field             => 'node server',
                             status            => $status,
                             message_tag       => 'NODE_SERVER_STATUS',
                             message_arguments => $msg_args,
                             target_name       => 'node monitor',
                             target_type       => $host,
                           }, };
    $self->insert_raw_record($record);

    say "set alert ", Data::Dumper::Dumper( [$record] )
        if $self->verbose;
    return;
}

# ----------------------------------------------------------------------
# Convenience routines to classify server status.
#
sub status_host_is_down {
    my $self = shift;
    return $self->status eq 'host down';
}

# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
sub status_host_has_power {
    my $self = shift;
    return $self->status eq 'host has power';
}

# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
sub status_host_is_pingable {
    my $self = shift;
    return $self->status eq 'host is pingable';
}

# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
sub status_host_timed_out {
    my $self = shift;
    return $self->status eq 'reboot timed out';
}

# ----------------------------------------------------------------------
# The core of NodeMonitor. Keep looping until our own health is OK, on
# the assumption that the dashboard and node server experience similar
# conditions in the rack. Once we are ok, verify the node server is
# down and try to relaunch it. Report to dashboard via the alerts
# table whether we succeed or time out waiting for server to power up.
# In case of success, exit, otherwise keep trying.
#
sub loop_core {
    my $self = shift;

    if ( $self->healthfile_status_shows_ok ) {
        $self->verify_host_down
            unless $self->status;

        $self->relaunch_host
            if $self->status_host_is_down;

        $self->ping_host_until_OK_or_timeout
            if $self->status_host_has_power;

        $self->report_host_status_to_boss
            if $self->status_host_timed_out
            || $self->status_host_is_pingable;

        if ( $self->status_host_is_pingable ) {
            say "NodeMonitor successfully rebooted @{[$self->host()]} @ ",
                strftime "%F %T", localtime;
            exit;    # All done, either success or failure!
        }
    }
    return;
}

# ======================================================================
1;
__END__

# ======================================================================
# POD

=head1 NAME

     An::NodeMonitor.pm - Resuscitate a failed server.

=head1 VERSION

This document describes An::NodeMonitor.pm version 1.0.0

=head1 SYNOPSIS

    use AN::NodeMonitor;

    my $nm = AN::NodeMonitor->new( { host => $host_to_resuscitate,
                                     healthfile => $path_to_file,
                                   } );
    $nm->run;

=head1 DESCRIPTION

This module implements the AN::NodeMonitor class, which is invoked
when a server is down. Its task is to restart the server and keep the
dashboard system notified of the current status of the server.

When the program starts up, it keeps looping until our own health is
OK, on the assumption that the dashboard and node server experience
similar conditions in the rack. Once we are ok, verify the node server
is down and try to relaunch it. Report to dashboard via the alerts
table whether we succeed or time out waiting for server to power up.
In case of success, exit, otherwise keep trying.

=head1 METHODS

The AN::NodeMonitor class is simply instantiated and run.

=head1 DEPENDENCIES

=over 4

=item B<AN::Cluster>

Provides the read_node_cache() routine to read cache file.

=item B<Carp> I<core>

Report errors as if they occur at call site.

=item B<Class::Tiny>

A simple OO framework. "Boilerplate is the root of all evil"

=item B<Const::Fast>

Provides fast constants.

=item B<English> I<core>

Provides meaningful names for Perl 'punctuation' variables.

=item B<File::Basename> I<core>

Parses paths and file suffixes.

=item B<POSIX> I<core>

Provides the strftime date-time formatting routine.

=item B<version> I<core>

Parses version strings.

=back

=head1 LICENSE AND COPYRIGHT

This program is part of Aleeve's Anvil! system, and is released under
the GNU GPL v2+ license.

=head1 BUGS AND LIMITATIONS

We don't yet know of any bugs or limitations. Report problems to 

    Alteeve's Niche!  -  https://alteeve.ca

No warranty is provided. Do not use this software unless you are
willing and able to take full liability for its use. The authors take
care to prevent unexpected side effects when using this
program. However, no software is perfect and bugs may exist which
could lead to hangs or crashes in the program, in your cluster and
possibly even data loss.

=begin unused

=head1  INCOMPATIBILITIES

There are no current incompatabilities.


=head1 CONFIGURATION

=head1 EXIT STATUS

=head1 DIAGNOSTICS

=head1 REQUIRED ARGUMENTS

=head1 USAGE

=end unused

=head1 AUTHOR

    Alteeve's Niche!  -  https://alteeve.ca

    Tom Legrady          December 2014
    -  tom@alteeve.ca
    -  tom@tomlegrady.com
=cut

# End of File
# ======================================================================


