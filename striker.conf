###############################################################################
#                                                                             #
# Alteeve's Niche!                                      Striker Configuration #
#                                                                             #
# This is the global configuration for the Striker dashboard and ScanCore     #
# alert system. Once configured on a dashboard, you can copy it as-is to your #
# Anvil! nodes.                                                               #
#                                                                             #
###############################################################################

### Common - These values apply to all utilities.

# This configures how email is sent from this machine. Please be sure to set
# the values below to the values given to you by your mail administrator (if
# that's not you).
#
# This is the mail server to connect to when sending email.
smtp::server			=	mail.example.com

# This is the TCP port used to connect to the mail server.
smtp::port			=	587

# This is the email address/IMAP user to send to the mail server. It is also
# the address used in the "From: " field for emails sent from this machine.
# NOTE: If you IMAP user name is not an email address, this may not work (it
# has not been tested). If you have trouble, please let us know:
# - https://alteeve.ca/w/Support
smtp::username			=	alert@example.com

# This is the password used for the email account above.
smtp::password			=	secret

# This is the security level to use when connecting to the peer, if needed.
smtp::security			=	STARTTLS

# This tells us to use an encrypted connection to the mail server. Set to '0'
# to send email plain-text (not recommended!).
smtp::encrypt_pass		=	1

# This is the domain given to the mail server when establishing the connection.
smtp::helo_domain		=	example.com

# This sets the "sending domain" when sending email. It should match
# smtp::server in most cases.
mail_data::sending_domain	=	example.com


##############################################################################
# ScanCore configuration variables                                           #
##############################################################################

# Database connection variables.
# 
# Hostname or IP of the Striker dashboard with the database.
#scancore::db::X::host
# 
# TCP port used to connect to the postgres server on the host.
#scancore::db::X::port
# 
# ScanCore database name
#scancore::db::X::name
# 
# ScanCore database user
#scancore::db::X::user
#
# ScanCore database password
#scancore::db::X::password
#
# Force the creation of the database, if needed.
#scancore::db::X::initialize
# 
scancore::db::1::host			=	an-striker01.alteeve.ca
scancore::db::1::port			=	5432
scancore::db::1::name			=	scancore
scancore::db::1::user			=	admin
scancore::db::1::password		=	Initial1
scancore::db::1::initialize		=	1

scancore::db::2::host			=	an-striker02.alteeve.ca
scancore::db::2::port			=	5432
scancore::db::2::name			=	scancore
scancore::db::2::user			=	admin
scancore::db::2::password		=	Initial1
scancore::db::2::initialize		=	1


# ScanCore can trigger a shutdown of a node for two reasons; Overheating and
# insufficient remaining runtime in batteries. The former protects your nodes
# from potential damage. Both provide a mechanism for gracefully shutting down
# the hosted virtual machines gracefully and cleanly powering down the nodes
# *before* they would shut down anyway from loss of power or hardware-based
# over-temperature shutdown.
# 
# Obviously, this means that ScanCore can potentially screw up and take the
# nodes offline if there was a bug. We don't pretend to be perfect.
# 
# If you want to disable this automatic shutdown feature, you can do so by 
# setting these variables to '1'.
scancore::disable::power_shutdown	=	0
scancore::disable::thermal_shutdown	=	0


###############################
# Power Shutdown Variables    #
###############################

# ScanCore will power down an Anvil! when the UPSes powering it lose input 
# power and drain below a minimum estimated hold up time. When the power is
# restored, the nodes will be booted once one of the UPSes has a minimum
# charge percentage.
# 
# By default, the minimum hold-up time when running on batteries is set to
# 10 minutes (600 seconds). The default minimum charge percentage to boot back
# up is 45%. 
# 
# You may want to adjust this to better suit your needs, however. 
# 
# Your minimum hold-up time should be the time it takes to perform a 
# "Cold Shut Down" from Striker, with all of your servers running, plus a
# little extra time as a buffer. This will ensure that your Anvil! will safely
# power down before the UPSes completely deplete.
# 
# The minimum charge percentage should be set to a high enough number to handle
# losing powert during the boot process. To know this percentage depends a lot
# on your environment. To calculate it, look at the load on your UPSes under
# normal operation (as a number of watts). Then consult your UPS's "Runtime" or
# "Hold-Up Time" chart. This will tell you how many minutes your UPS will run 
# given your load and a full charge. Divide the time you need to fully boot and
# then shut down your Anvil! by the runtime at full charge for your load. 
# Multiply the result by 100 to get your minimum charge percentage.
# 
# For example;
# 
# Assume you need 15 minutes at worst from the time you start to boot your 
# Anvil! until you can shut it back down. Also assume you have an average 400w
# load on an APC SmartUPS 1500 UPS.
# 
# Looking at the "Batteries & Runtime" chart here:
# http://www.apc.com/resource/include/techspec_index.cfm?base_sku=SMT1500RM2U
# We see that a full charge can hold up a 400w load for 37 minutes. 
# 
# Knowing this, we have: ((15 % 37) x 100) == 40.5%. We'll pad this to 45%
# 
# To configure this manually, uncomment the lines below. The 
# 'scancore::minimum_ups_runtime' is the number of seconds and 
# 'scancore::minimum_safe_charge' is the percentage (without the % sign).
#
#scancore::minimum_ups_runtime		=	600
#scancore::minimum_safe_charge		=	45


###############################
# Thermal Shutdown Variables  #
###############################

# Each sensor has a default "weight" of '1'. When ScanCore finishes a cycle,
# it will look at the number of sensors that have gone above their high 
# critical threshold, or below their low critical threshold, and add up their
# weights. If the total value is equal to or greater than this shutdown limit,
# ScanCore will initiate a withdrawl and power down of the node.
scancore::temperature::shutdown_limit	=	5

# When a node goes into thermal shutdown, many of it's temperature (and other)
# sensors go offline. So a dashboard has a limited ability to determine if it
# is safe to boot a node back up. 
# 
# To account for this, when a node is in a thermal emergency stop, the 
# dashboard will check it's own temperature (if it has temp sensors) and if it
# is OK, it will read the node's 'Ambient' and 'Systemboard' (or the sensors
# defined by you in 'scan-ipmitool::offline_sensor_list' (or similar from other
# scan agents). 
# 
# If those sensors on the target node are OK, then the last step is to count
# how many times in the last six hours the node went into thermal shutdown. The
# more times it has gone into thermal shutdown, the longer it waits before
# booting the node. By default, the delays are:
# 
# Reboots | Wait X seconds until boot
# --------+--------------------------
#  1      | 600   (10 minutes)
#  2      | 1800  (30 minutes)
#  3      | 3600  (1 hour)
#  4      | 7200  (2 hours)
#  >4     | 21600 (6 hours)
# --------+--------------------------
# 
# The goal here is to minimize the risk of damage to the hardware. ScanCore
# tries to shutdown a node before it goes into hardware-thermal shutdown and
# before damage can occur, but it's not perfect. It is possible that thermal
# shutdown is disabled in the BIOS and that the graceful shutdown of the node
# could take too long and damage could occur. This increasing delay is 
# designed to help minimize the risk of a chronic failure in the node causing
# the node to repeatedly be put at risk.
# 
# The trade off, of course, is increased downtime. Particularly if both nodes
# went into shutdown. So it is possible for you to override the default timing
# using the variables below.
#scancore::thermal_reboot_delay::1	=	600
#scancore::thermal_reboot_delay::2	=	1800
#scancore::thermal_reboot_delay::3	=	3600
#scancore::thermal_reboot_delay::4	=	7200
#scancore::thermal_reboot_delay::more	=	21600


##############################################################################
# ScanCore - Scan Agent Overrides                                            #
##############################################################################

### scan-ipmitool
# Where possible, the upper and lower temperature sensor values are pulled from
# the IPMI sensor data itself. Some sensor values do not specify their upper
# and lower sensor values, however. In this case, scan-ipmitool uses the 
# following default limits (all in °C!):
# high_warning	=	50
# high_critical	=	55
# low_warning	=	5
# low_critical	=	0
# 
# To catch sudden increases in temperature, usually indicative of an AC
# failure, we set a "jump" of 'delta' value. If the temperature of a sensor
# increases by more than this number of degrees celsius, a "sudden change"
# alert will be generated.
# The default 'jump' value is 5°C between two scans, typically 30 seconds or
# so.
# 
# To avoid repeated alerts when a thermal sensor is hovering around a 
# threshold, a buffer is used. For an alert to clear, a sensor must drop or 
# rise 2*C below or above and high or low threshold, respectively. For example,
# if a sensor goes into warning at 50*C, it must drop below 48*C for the alert
# to be cleared.
# 
# If you want to use a different set of defaults, you can do so by uncommenting
# and editing the following variables:
#scan-ipmitool::thresholds::default::high_warning	=	50
#scan-ipmitool::thresholds::default::high_critical	=	55
#scan-ipmitool::thresholds::default::low_warning	=	5
#scan-ipmitool::thresholds::default::low_critical	=	0
#scan-ipmitool::thresholds::default::jump		=	5
#scan-ipmitool::thresholds::default::buffer		=	2

# To override the thresholds of a single sensor, you can do so using the sensor
# name reported by 'ipmitool'. To get a list of the sensors and their names,
# you can use the command (from a linux machine):
# 
# ipmitool -H an-a05n01.ipmi -U admin sensor list all
# 
# Replace 'an-a05n01.ipmi' with the hostname or IP of your IPMI device.
# Replace 'admin' with the IPMI user name for your IPMI device.
# Enter the IPMI user's password when prompted.
#
# NOTE: Be sure to match the sensor name exactly!
# 
# As an example, if you wanted to manually adjust the threshold 'Ambient' 
# threshold, you can use the following;
#scan-ipmitool::thresholds::Ambient::high_warning	=	50
#scan-ipmitool::thresholds::Ambient::high_critical	=	55
#scan-ipmitool::thresholds::Ambient::low_warning	=	5
#scan-ipmitool::thresholds::Ambient::low_critical	=	0
#scan-ipmitool::thresholds::Ambient::jump		=	5
#scan-ipmitool::thresholds::Ambient::buffer		=	2
#
# You can adjust the weight of a given sensor (up or down) using 'weight',
# expressed as a whole or real number.
#scan-ipmitool::thresholds::Ambient::weight		=	1.5


# When a node is shut down because the temperature got too high or low, most
# of the thermal sensors stop reporting data. In most cases, though, the
# 'Ambient' and 'Systemboard' remain readable. By default, the dashboard will
# check it's own temperature and, if that is OK, check these sensors on the
# target node to determine if the node is safe to boot back up. If your nodes
# use different names, or if you have access to additional sensors, you can
# change the list of sensors checked by specifying them here as a 
# comma-separated list.
# 
# NOTE! The sensor names are case-sensitive and must match exactly to the 
#       output show in ipmitool!
#
# To determine which sensor values are available when your nodes are off, you
# can run the following command from the Striker dashboard:
# 
# ipmitool -H <node_ipmi_ip> -U <ipmi_user> sensor list all | grep 'degrees C'
#
#==== 
#Ambient          | 25.000     | degrees C  | ok    | na        | 1.000     | 6.000     | 37.000    | 42.000    | na        
#Systemboard      | 35.000     | degrees C  | ok    | na        | na        | na        | 65.000    | 70.000    | na        
#==== 
# If you have different hardware nodes, and the sensors differ between them,
# list all of the sensors here. Once that are not found will be ignored.
#scan-ipmitool::offline_sensor_list			=	Ambient,Systemboard


# Most Striker Dashboards run commodity hardware and, thus, do not have IPMI.
# When a dashboard does have IPMI, it can make a much more informed decision
# about whether it is safe to boot a node that has gone into thermal shutdown
# because it will be able to check all of it's own sensors. If any are in a
# 'warning' state, it will not boot the nodes. This is excellent when dealing
# with a data center or room that has lost cooling.
# 
# If your dashboards do have IPMI, you can tell scan-ipmitool about how to 
# access it using:
# 
#scan-ipmitool::machine::<hostname>::power_check_command = <fence_ipmilan call>
# 
# The 'fence_ipmilan' command is the command used by Striker dashboards to
# check the state of and to control the nodes. Generally, the command looks
# like this:
# 
#fence_ipmilan -a <ipmi_ip> -l <ipmi_user> -p <password> -o status
# 
# Try this call out on the dashboard to see if you can get the power state of
# the machine. It should return:
# 
#Status: ON
# 
# If it doesn't, please read 'man fence_ipmilan' to see what other switches you
# might need to be able to read it. 
# 
# Once you can read the status, remove the '-o status' and use the rest of the
# command as the value for this variable. Here is an example for the Striker
# dashboard called 'an-striker01.alteeve.ca':
# 
#scan-ipmitool::machine::an-striker01.alteeve.ca::power_check_command	=	fence_ipmilan -a an-striker01.ipmi -l admin -p Initial1


##############################################################################
# ScanCore Alert Targets                                                     #
##############################################################################

# This controls who gets alerts. The available alert options are:
# 
# - debug;    Useful for trying to solve problems and should only be logged.
#             This will generate an alert on just about every state change.
# - info;     This level will generate an alert on events that are almost
#             always harmless.
# - notice;   This is the lowest alert level that most people will actually
#             want to use. It includes events like migrations, servers booting,
#             BBU/FBU self tests, etc.
# - warning;  Most technical staff will want to receive these alerts. Any state
#             change that could indicate an interruption will use this alert
#             level.
#             NOTE: This is the default level
# - critical; This alert level will be of interest to non-technical users, like
#             managers. Alerts at this level almost always indicate a serious
#             event has occured, like a node being lost/fenced, thermal shut
#             down occuring and so forth.
#
# A user who is listening to a given alert level will also receive all higher
# level warnings.
# 
# Alert recipients can either be 'email' or 'file'. The 'email' type are meant
# for humans to receive and are dispatched via a local postfix relay. The
# 'file' type records alerts to the specified file which sits on the local
# disk until it is collected by a person or program.
# 
# By default, metric units are used in reporting values. If the user prefers
# imperial measurements (ie: Fahrenheit instead of Celsius for temperatures),
# you can use 'units=imperial'.
# 
# E-mail recipients are entered in the following format:
#alerts::recipient::1::email		=	name="Madison Kelly",email="mkelly@alteeve.ca",language="en_CA",level="notice"
#
# File recipients are entered in the following format, with "imperial"
# measurements selected as an example:
#alerts::recipient::2::file		=	file="/var/log/alerts.en_CA.log",languagge="en_CA",level="info",units=imperial
#
# What is important is that the integer after 'alerts::recipient::X' is unique.
# 
# NOTE: If a language is selected but not supported, the alert will fall back
#       to 'en_CA'. Please be sure all the scan agents you plan to use, along
#       with ScanCore itself, have the appropriate language strings before you
#       use them!
# 
alerts::recipient::1::email		=	name="Madison Kelly",email="debug@alteeve.ca",language="en_CA",level="notice"
alerts::recipient::2::file		=	file="/var/log/alerts.en_CA.log",languagge="en_CA",level="info",units=imperial


###############################################################################
# Striker USB management                                                      #
###############################################################################

### Overview
# 
# This control how USB mass storage devices will be managed when plugged into
# given physical USB ports. It provides a mechanism for mounting a USB drive on
# a server running linux as though the USB drive had been plugged into the 
# server directly, from the user's perspective.
# 
# It works by mounting a USB storage locally, then connecting to the target
# server and uses sshfs to mount it on the target. In this way, the drive will
# appear automatically on the user's desktop (or at a defined mount point)
# automatically, and clean itself up automatically, when a USB drive is
# inserted or removed from the host.
# 
# It also supports LUKS encrypted USB drives, including the ability to setup
# encryption on the USB drive if it is not yet encrypted, for you. This does
# require storing the USB drive's passphrase on this machine, so caution is
# required when used this way.

### Variables
#
# When it's time to mount the local USB drive's mount point on a remote
# machine, we need to pass credentials to the remote machine in order for it
# to connect to this machine.
#
# The host name entered below must be resolvable on the target server. If in
# doubt, use this machine's IP address.
# 
# NOTE: If you use a host name, make sure the remote machine can resolve it to
#       an IP address!
# 
local::host			=	10.255.4.3
local::user			=	root
local::password			=	secret

# This is set to '1' when you want to enable this feature. When using 
# auto-encryption, be careful to only enable this when there is low risk of a
# user accidentally inserting a USB drive they don't want reformated!
local::enable_remote_usb_mount	=	1

# It is possible to decrypt LUKS-encrypted partition, provided the LUKS key is
# provided. At this time, only one global LUKS key is supported. Enter it in
# the following variable.
luks::passphrase		=	supersecret

# If the partition is not 'ext4' or if it's not encrypted, setting this to '1'
# will cause the drive to be reformatted and encrypted without prompt. This
# should only be enabled in very specific circumstances. If used, be sure to
# warn users that their drives will be reformatted if they plug them into a
# managed port and they are not encrupted!
luks::force_initialize		=	1

# This tells us what file system to create on the decrypted device mapper
# device after we created the LUKS encrypted partition.
luks::use_filesystem		=	ext4

# Set this below if you want to define a specific label when formatting a newly
# encrypted device with the filesystem set above. If your selected filesystem
# doesn't support labels, leave this blank.
luks::fs_label			=	nc0
luks::fs_options		=	-L #!variable!fs_label!# 

# If the 'force_initialize' option is enabled, you can use this option to
# selectively block the forced reformatting based on the label of the drive.
# This can be a static name or a basic regular expression. If the value ends
# in '*', then a label will be considered a match if the start of the string
# matches the value to the left of the '*' here. The string *is* case 
# sensitive.
luks::protected_label		=	c*


###############################################################################
# Anvil! definitions                                                          #
###############################################################################

# This defines the cluster. If you want to defined multiple clusters for
# Striker, copy an entry in this section and increment the variable integer.
# The order and value do not matter. It's simply a differentiator. If only one
# entry exists, the Dashboard's Anvil! selection screen will not show and the
# sole entry will be auto-selected.

# Variables are:
# name:        Must match the name set in the Anvil!'s "cluster.conf" file. 
# nodes:       A comma-separated list of nodes. On the Striker server, this
#              must be resolvable to the target machine (ie: via /etc/hosts +
#              ~/.ssh/config)
# company:     A free-form field used to show the owner of the Anvil!. 
#              Particularly useful for resellers and companies with divisions.
# description: Also a free-form field used to describe the purpose of the
#              particular Anvil!.
# url:         If defined, a link to the given URL will be shown beside the
#              Anvil!'s name in the Dashboard's Anvil! selection screen. Useful
#              for linking to internal documentation or similar.
# ricci_pw:    This is the password used by the 'ricci' user on the Anvil!'s
#              nodes. It must be set accurately in order to add or remove
#              servers. If a password is needed to log into one of the nodes,
#              this password will be used. If this doesn't work, you will need
#              to manually add the dashboard's public key to each node's root
#              user's 'authorized_keys' file.
