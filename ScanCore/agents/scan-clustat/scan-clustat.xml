<?xml version="1.0" encoding="UTF-8"?>
<!--
This strings file is for the rgmanager 'clustat' Scan Agent. Please be careful
not to use names used by the main Striker strings file. Do this by prefixing
all strings with 'scan_clustat_'.
-->
<strings>
	<!-- Canadian English -->
	<name>scan-clustat.xml</name>
	<version>0.0.001</version>
	<langs>en_CA</langs>
	<!-- Canadian English -->
	<lang name="en_CA" long_name="English (Canadian)">
		<key name="comment">Maintained by Madison Kelly (mkelly@alteeve.ca) for the rgmanager 'clustat' Scan Agent.</key>
		
		<!-- Strings that can change by rebranding -->
		<key name="scan_clustat_brand_0001">'clustat' Scan Agent</key>
		
		<!-- Log messages -->
		<key name="scan_clustat_log_0001">This node is offline. No checks of 'clustat' nodes or services will be made.</key>
		
		<!-- Notices -->
		<key name="scan_clustat_note_0001">It appears that the 'clustat' node: [#!variable!node!#] was deleted.</key>
		<key name="scan_clustat_note_0002">It appears that the 'clustat' service: [#!variable!service!#] was deleted.</key>
		<key name="scan_clustat_note_0003">It appears that the server: [#!variable!service!#] was deleted.</key>
		<key name="scan_clustat_note_0004">The new node: [#!variable!node!#] has appeared.</key>
		<key name="scan_clustat_note_0005">
It looks like 'rgmanager' (the cluster's resource manager) was stopped on the node: [#!variable!node!#]. 
It is still an #!string!brand_0004!# member, but it can no longer recover lost servers.
- NOTE: If this node's rgmanager has stopped, it's view of the peer's state may be inaccurate.
		</key>
		<key name="scan_clustat_note_0006">The node has joined the cluster stack: [#!variable!cluster!#].</key>
		<key name="scan_clustat_note_0007">The cluster manager was stopped on the node: [#!variable!node!#]. The node is no longer participating in the #!string!brand_0004!#.</key>
		<key name="scan_clustat_note_0008">The node: [#!variable!node!#] has rejoined the #!string!brand_0004!#, but it is not yet able to take over lost servers. It should be ready in the next minute or so.</key>
		<key name="scan_clustat_note_0009">
The node: [#!variable!node!#] is now a full member of #!string!brand_0004!#. 
As soon as its storage is 'UpToDate', it will be ready to take over servers.
- NOTE: If this node is the one that joined, it's view of the peer's state may be inaccurate.
		</key>
		<key name="scan_clustat_note_0010">
The node: [#!variable!node!#] has fully rejoined the #!string!brand_0004!#. 
As soon as its storage is 'UpToDate', it will be ready to take over servers.
- NOTE: If this node is the one that joined, it's view of the peer's state may be inaccurate.
		</key>
		<key name="scan_clustat_note_0011">The service: [#!variable!service!#] is starting on the host: [#!variable!host!#]...</key>
		<key name="scan_clustat_note_0012">The node: [#!variable!node!#]'s internal ID number has changed from: [#!variable!old_id!#] to:  [#!variable!new_id!#]. This should never unless an admin changed this intentionally.</key>
		<key name="scan_clustat_note_0013">The new service: [#!variable!service!#] has appeared.</key>
		<key name="scan_clustat_note_0014">Congratulations! Your new server: [#!variable!service!#] has been created.</key>
		<key name="scan_clustat_note_0015">The server: [#!variable!service!#] is booting on the host: [#!variable!host!#]...</key>
		<key name="scan_clustat_note_0016">The server: [#!variable!service!#] has booted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0017">The node has gained quorum. This means that the node can now participate in the #!string!brand_0004!# again.</key>
		<key name="scan_clustat_note_0018">The server: [#!variable!service!#] is rebooting on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0019">The server: [#!variable!service!#] is being stopped on the host: [#!variable!host!#]. Please make sure the OS initiated power off!</key>
		<key name="scan_clustat_note_0020">The server: [#!variable!service!#] has been stopped.</key>
		<key name="scan_clustat_note_0021">The server: [#!variable!service!#] has been restarted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0022">The service: [#!variable!service!#] has started on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0023">The server: [#!variable!service!#] is rebooted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0024">The server: [#!variable!service!#] has shutdown.</key>
		<key name="scan_clustat_note_0025">The service: [#!variable!service!#] is being stopped on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0026">The service: [#!variable!service!#] has been stopped.</key>
		<key name="scan_clustat_note_0027">All servers are now running on: [#!variable!node!#], so this node has been designated as the primary node.</key>
		<key name="scan_clustat_note_0028">The server: [#!variable!service!#] has been migrated from: [#!variable!source!#] to: [#!variable!target!#].</key>
		<key name="scan_clustat_note_0029">The server: [#!variable!service!#] recovered from a failed state.</key>
		<key name="scan_clustat_note_0030">The server: [#!variable!service!#] is off, so its failed state was cleared. It is now in the disabled state.</key>
		<key name="scan_clustat_note_0031">There are two or more 'clustat_uuid' entries for this host. This should not happen and is the result of a program issue. We'll fix this, but it might trigger additional alerts as services may detect as found or changed.</key>
		<key name="scan_clustat_note_0032">The 'clustat_uuid': [#!variable!clustat_uuid!#] has the most services and nodes referencing it, so we'll preserve this one and delete the rest.</key>
		<key name="scan_clustat_note_0033">Removing any references to the 'clustat_uuid': [#!variable!clustat_uuid!#].</key>
		
		<!-- Warnings. -->
		<key name="scan_clustat_warning_0001">#!free!#</key>
		<key name="scan_clustat_warning_0002">#!free!#</key>
		<key name="scan_clustat_warning_0003">#!free!#</key>
		<key name="scan_clustat_warning_0004">#!free!#</key>
		<key name="scan_clustat_warning_0005">#!free!#</key>
		<key name="scan_clustat_warning_0006">#!free!#</key>
		<key name="scan_clustat_warning_0007">#!free!#</key>
		<key name="scan_clustat_warning_0008">#!free!#</key>
		<key name="scan_clustat_warning_0009">
The cluster's state has changed in an unexpected way.
- Cluster name:  [#!variable!old_cluster_name!#] -> [#!variable!new_cluster_name!#]
- Quorum status: [#!variable!old_quorum_status!#] -> [#!variable!new_quorum_status!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0010">
The server: [#!variable!service!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0011">
The node: [#!variable!node!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0012">
The service: [#!variable!service!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0013">The server: [#!variable!service!#] failed to boot. Recovery will be attempted now...</key>
		<key name="scan_clustat_warning_0014">
The server: [#!variable!service!#] has entered a failed state. 
This can happen if the server took more than 2 minutes to stop after a graceful shutdown request. 
That can happen because the server is installing updates, a program blocked the shutdown or it ignored the request to shut down.
Connect to the server and shut it down manually please.
		</key>
		<key name="scan_clustat_warning_0015">The service: [#!variable!service!#] failed to start!</key>
		<key name="scan_clustat_warning_0016">The service: [#!variable!service!#] has failed!</key>
		<key name="scan_clustat_warning_0017">The service: [#!variable!service!#] has been restarted on the host: [#!variable!host!#]. This is generally not normal, unless someone is doing work on a node.</key>
		<key name="scan_clustat_warning_0018">The node: [#!variable!node!#] has gone offline. This can be caused by someone is doing work on your #!string!brand_0004!#, or if a node was withdrawn to shed load, or if the node's health went critical. If this was not expected, the node may have crashed and been fenced.</key>
		<key name="scan_clustat_warning_0019">The server: [#!variable!service!#] was disabled after being marked as failed.</key>
		<key name="scan_clustat_warning_0020">The server: [#!variable!service!#] was recovered successfully on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_warning_0021">The server: [#!variable!service!#] was disabled after being marked as failed.</key>
		<key name="scan_clustat_warning_0022">The server: [#!variable!service!#] was recovered successfully on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_warning_0023">The node has left the cluster: [#!variable!cluster!#].</key>
		<key name="scan_clustat_warning_0024">
The node has lost quorum. 
This means that the node can no longer participate in the #!string!brand_0004!#. 
This should never happen. 
#!string!scancore_brand_0005!#
		</key>
		<key name="scan_clustat_warning_0025">There are no servers running on the peer node, and we have: [#!variable!servers!#] hosted here. The fence delay does not favour us, so that will now be fixed.</key>
		<key name="scan_clustat_warning_0026">The server: [#!variable!server!#] was in a failed state, and it is not running on either node now, so the 'failed' state was cleared.</key>
		<key name="scan_clustat_warning_0027">The server: [#!variable!service!#] failed to stop in time after a request to shut down. It is now marked as 'failed', which will be cleared once it does finally shut down. Manual shut down is likely required.</key>
		
		<!-- clustat Agent-specific messages -->
		<key name="scan_clustat_message_0001">Starting #!string!scan_clustat_brand_0001!#:</key>
		<key name="scan_clustat_message_0002"><![CDATA[
 -=] ScanCore - Agent - scan-clustat

DESCRIPTION

This agent reads and parses the output from rgmanager's 'clustat' (Cluster
Status) program. It will detect changes in VMs, storage, libvirtd services and
nodes joining, leaving and recovering.


SWITCHES


 -c <host>, --caller <host>
	
	This is to be set by the ScanCore server and 'host' must match the
	FQDN of the caller. When data is recorded to the target databases, this
	value is used to link the data to this host. Failing to match this name
	to an entry in the 'hosts' table will cause the agent to exit with the
	return code of '1'.

 -h, -?, --help

	Show this dialog and exit.

 --prep-db

	If passed, the schema will be loaded into the database (if not 
	already), even if 'clustat' is not found. Note: This won't work if the
	agent has been manually disabled via 'scan-clustat::disable'.

                  
SUPPORT

https://alteeve.com/w/Support                             Alteeve's Niche! Inc.
		]]></key>
		<key name="scan_clustat_message_0003">It looks like 'rgmanager' is not in installed, exiting.</key>
	</lang>
</strings>
