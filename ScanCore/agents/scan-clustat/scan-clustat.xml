<?xml version="1.0" encoding="UTF-8"?>
<!--
This strings file is for the rgmanager 'clustat' Scan Agent. Please be careful
not to use names used by the main Striker strings file. Do this by prefixing
all strings with 'scan_clustat_'.
-->
<strings>
	<!-- Canadian English -->
	<name>scan-clustat.xml</name>
	<version>0.0.001</version>
	<langs>en_CA</langs>
	<!-- Canadian English -->
	<lang name="en_CA" long_name="English (Canadian)">
		<key name="comment">Maintained by Madison Kelly (mkelly@alteeve.ca) for the rgmanager 'clustat' Scan Agent.</key>
		
		<!-- Strings that can change by rebranding -->
		<key name="scan_clustat_brand_0001">'clustat' Scan Agent</key>
		
		<!-- Log messages -->
		<key name="scan_clustat_log_0001">This node is offline. No checks of 'clustat' nodes or services will be made.</key>
		
		<!-- Notices -->
		<key name="scan_clustat_note_0001">It appears that the 'clustat' node: [#!variable!node!#] was deleted.</key>
		<key name="scan_clustat_note_0002">It appears that the 'clustat' service: [#!variable!service!#] was deleted.</key>
		<key name="scan_clustat_note_0003">It appears that the server: [#!variable!service!#] was deleted.</key>
		<key name="scan_clustat_note_0004">The new node: [#!variable!node!#] has appeared.</key>
		<key name="scan_clustat_note_0005">It looks like 'rgmanager' (the cluster's resource manager) was stopped on the node: [#!variable!node!#]. It is still an #!string!brand_0004!# member, but it can no longer recover lost servers.</key>
		<key name="scan_clustat_note_0006">The node has joined the cluster stack: [#!variable!cluster!#].</key>
		<key name="scan_clustat_note_0007">The cluster manager was stopped on the node: [#!variable!node!#]. The node is no longer participating in the #!string!brand_0004!#.</key>
		<key name="scan_clustat_note_0008">The node: [#!variable!node!#] has rejoined the #!string!brand_0004!#, but it is not yet able to take over lost servers. It should be ready in the next minute or so.</key>
		<key name="scan_clustat_note_0009">The node: [#!variable!node!#] is now a full member of #!string!brand_0004!#. As soon as its storage is 'UpToDate', it will be ready to take over servers.</key>
		<key name="scan_clustat_note_0010">The node: [#!variable!node!#] has fully rejoined the #!string!brand_0004!#. As soon as its storage is 'UpToDate', it will be ready to take over servers.</key>
		<key name="scan_clustat_note_0011">The service: [#!variable!service!#] is starting on the host: [#!variable!host!#]...</key>
		<key name="scan_clustat_note_0012">The node: [#!variable!node!#]'s internal ID number has changed from: [#!variable!old_id!#] to:  [#!variable!new_id!#]. This should never unless an admin changed this intentionally.</key>
		<key name="scan_clustat_note_0013">The new service: [#!variable!service!#] has appeared.</key>
		<key name="scan_clustat_note_0014">Congratulations! Your new server: [#!variable!service!#] has been created.</key>
		<key name="scan_clustat_note_0015">The server: [#!variable!service!#] is booting on the host: [#!variable!host!#]...</key>
		<key name="scan_clustat_note_0016">The server: [#!variable!service!#] has booted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0017">The node has gained quorum. This means that the node can now participate in the #!string!brand_0004!# again.</key>
		<key name="scan_clustat_note_0018">The server: [#!variable!service!#] is rebooting on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0019">The server: [#!variable!service!#] is being stopped on the host: [#!variable!host!#]. Please make sure the OS initiated power off!</key>
		<key name="scan_clustat_note_0020">The server: [#!variable!service!#] has been stopped.</key>
		<key name="scan_clustat_note_0021">The server: [#!variable!service!#] has been restarted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0022">The service: [#!variable!service!#] has started on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0023">The server: [#!variable!service!#] is rebooted on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0024">The server: [#!variable!service!#] has shutdown.</key>
		<key name="scan_clustat_note_0025">The service: [#!variable!service!#] is being stopped on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_note_0026">The service: [#!variable!service!#] has been stopped.</key>
		<key name="scan_clustat_note_0027">All servers are now running on: [#!variable!node!#], so this node has been designated as the primary node.</key>
		<key name="scan_clustat_note_0028">The server: [#!variable!service!#] has been migrated from: [#!variable!source!#] to: [#!variable!target!#].</key>
		
		<!-- Warnings. -->
		<key name="scan_clustat_warning_0001">#!free!#</key>
		<key name="scan_clustat_warning_0002">#!free!#</key>
		<key name="scan_clustat_warning_0003">#!free!#</key>
		<key name="scan_clustat_warning_0004">#!free!#</key>
		<key name="scan_clustat_warning_0005">#!free!#</key>
		<key name="scan_clustat_warning_0006">#!free!#</key>
		<key name="scan_clustat_warning_0007">#!free!#</key>
		<key name="scan_clustat_warning_0008">#!free!#</key>
		<key name="scan_clustat_warning_0009">
The cluster's state has changed in an unexpected way.
- Cluster name:  [#!variable!old_cluster_name!#] -> [#!variable!new_cluster_name!#]
- Quorum status: [#!variable!old_quorum_status!#] -> [#!variable!new_quorum_status!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0010">
The server: [#!variable!service!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0011">
The node: [#!variable!node!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0012">
The service: [#!variable!service!#]'s status has changed in an unexpected way.
- The old state was: [#!variable!old_state!#]
- The new state is:  [#!variable!new_state!#]

Please send this alert to: #!string!scancore_brand_0003!# (#!string!scancore_brand_0004!#) so that we can add this condition to the list of known alert types.
		</key>
		<key name="scan_clustat_warning_0013">The server: [#!variable!service!#] failed to boot. Recovery will be attempted now...</key>
		<key name="scan_clustat_warning_0014">The server: [#!variable!service!#] has failed!</key>
		<key name="scan_clustat_warning_0015">The service: [#!variable!service!#] failed to start!</key>
		<key name="scan_clustat_warning_0016">The service: [#!variable!service!#] has failed!</key>
		<key name="scan_clustat_warning_0017">The service: [#!variable!service!#] has been restarted on the host: [#!variable!host!#]. This is generally not normal, unless someone is doing work on a node.</key>
		<key name="scan_clustat_warning_0018">The node: [#!variable!node!#] has gone offline. This can be caused by someone is doing work on your #!string!brand_0004!#, or if a node was withdrawn to shed load, or if the node's health went critical. If this was not expected, the node may have crashed and been fenced.</key>
		<key name="scan_clustat_warning_0019">The server: [#!variable!service!#] was disabled after being marked as failed.</key>
		<key name="scan_clustat_warning_0020">The server: [#!variable!service!#] was recovered successfully on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_warning_0021">The server: [#!variable!service!#] was disabled after being marked as failed.</key>
		<key name="scan_clustat_warning_0022">The server: [#!variable!service!#] was recovered successfully on the host: [#!variable!host!#].</key>
		<key name="scan_clustat_warning_0023">The node has left the cluster: [#!variable!cluster!#].</key>
		<key name="scan_clustat_warning_0024">
The node has lost quorum. 
This means that the node can no longer participate in the #!string!brand_0004!#. 
This should never happen. 
#!string!scancore_brand_0005!#
		</key>
		<key name="scan_clustat_warning_0025">There are no servers running on the peer node, and we have: [#!variable!servers!#] hosted here. The fence delay does not favour us, so that will now be fixed.</key>
		
		<!-- clustat Agent-specific messages -->
		<key name="scan_clustat_message_0001">Starting #!string!scan_clustat_brand_0001!#:</key>
		<key name="scan_clustat_message_0002"><![CDATA[
 -=] ScanCore - Agent - scan-clustat

DESCRIPTION

This agent reads and parses the output from rgmanager's 'clustat' (Cluster
Status) program. It will detect changes in VMs, storage, libvirtd services and
nodes joining, leaving and recovering.


SWITCHES


 -c <host>, --caller <host>
	
	This is to be set by the ScanCore server and 'host' must match the
	FQDN of the caller. When data is recorded to the target databases, this
	value is used to link the data to this host. Failing to match this name
	to an entry in the 'hosts' table will cause the agent to exit with the
	return code of '1'.

 -h, -?, --help

	Show this dialog and exit.

 --prep-db

	If passed, the schema will be loaded into the database (if not 
	already), even if 'clustat' is not found. Note: This won't work if the
	agent has been manually disabled via 'scan-clustat::disable'.

                  
SUPPORT

https://alteeve.com/w/Support                             Alteeve's Niche! Inc.
		]]></key>
		<key name="scan_clustat_message_0003">It looks like 'rgmanager' is not in installed, exiting.</key>
	</lang>
</strings>
